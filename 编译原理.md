第一步：实现 json 解析器->实现 scheme 解析器，你应该基本入门了。

第二步：把教科书上的 pascal 版本的 PL/0 翻译成 C 或者python 并读懂，模仿写个别的，你应该能做一个 lua 解析器了。

第三步：实现 LL(0) 匹配，实现 LL(k) 匹配 -> 实现 LR 匹配，你应该能轻松实现些一个高级点的语法了。

[编程大佬不会写tokenizer是不是一件很值得骄傲的事情？ - 韦易笑的回答 - 知乎](https://www.zhihu.com/question/60726251/answer/180203853)

---

为什么网上各种教程都是教人写tokenize和parser，说明这部分太容易学了。

不吹不黑，一个数据结构及格的程序员，即使完全不懂编译原理，看一下nfa和递归下降的原理再写一个简单的tokenize和parser解析一个简单的语言两天应该能搞定。不过要是吃透编译器前端的那么多知识，比如nfa->dfa到最小dfa，LL/LR/SLR/LALR，能做到手写标准的正则引擎，山寨yacc的水平的话，不潜下心来钻研个一年半载估计是没戏。能做到这个水平，全国任何公司的offer肯定都是任意挑选的。反正我比较笨学了好多年都还没吃透

编译器前端的到目前为止确实像是到头了，像lex、yacc、antlr这么多工具你看看手册就能写个parser出来。而编译器后端的代码生成、数据流分析、寄存器分配这些怎么就没见多少人谈论？还不是因为这部分跟前端相比难得多，高手都直接发论文了，普通人接触过这些东西就会发现自己知识的浅薄不敢乱吹牛了。

[会写 Parser、Tokenizer 是什么水平？ - ototsuyume的回答 - 知乎](https://www.zhihu.com/question/30746665/answer/49283528)